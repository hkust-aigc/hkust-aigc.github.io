<!DOCTYPE html>
<html lang = "en-us">

<head>
  <meta charset="utf-8">
  <meta name = "description" content="A studio that shows videos produced by generative AI">
  <meta name = "keywords" content="Generative AI,video,github">
  <meta name = "viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=yes">
  <title> HKUST AIGC Studio</title>

  <link rel = "stylesheet" href = "/css/reset.css">
  <link rel = "stylesheet" href = "/css/pageStyle.css">
  <link rel = "stylesheet" href = "/css/topbar.css">
  <link rel = "stylesheet" href = "/css/tutorial.css">
  <link rel = "stylesheet" href = "/css/videoSytle.css">
</head>

<body>
  
  <!--top navigation-->

  <div class = "topbar">

    <div class = "container clearfix">
      <!--logo-->

      <div class = "topbar-icon leftfix">
        <a href = "/"> <img class = "logo" src="/images/logo.png" alt="HKUST AIGC logo"> </a>
      </div>

      <!--navigation-->

      <div class = "topbar-navigation rightfix">
        <ul class = "list clearfix">
          <li> 
            <a href= "/explore"> Explore</a> 
          </li>
          <li> 
            <a href="/showcase" > Showcase </a> 
          </li>
        </ul>

        <div class = "Creator">
          <a href="/videoCreators"> 
            <button> Create </button>
          </a> 
        </div>

        <div class = "searchBox">
          <form action = "/search/creations/">
            <input type = "text" name = "searchText" placeholder = "Search HKUST AIGC Studio">
            <button> search </button>
          </form> 
        </div>
        
      </div>

    </div>
  </div>

  <div class = "tutorial">

    <article> 
      <h2 class = "Title"> <span> Rerender A Video </span></h2>
      <h3 class = "Author"> <span>HO Shao Ping</span> </h3>

      <h3 class = "Subtitle"> 
        <span> Introduction </span>
      </h3>
      <p class = "Text"> 
        <span> 
          Text-to-video generation are capable of generating high-quality images. However, when applying these models to video domain, 
          ensuring temporal consistency across video frames remains a formidable challenge. Rerender A Video is a novel zero-shot text-guided 
          video-to-video translation framework that is able to adapt image models to videos while also customizing a specific subject with 
          LoRA, and introducing extra spatial guidance with ControlNet to rendering high-quality and temporally-coherent videos which don't 
          flicker. 
        <span>
          In this article, I will introduce the different parameters of this AI-powered tool to guide you through creating your own video. 
          To give a better understanding, I will be demonstrating the steps using Hugging Face Space at 
          <a href = "https://huggingface.co/spaces/Anonymous-sub/Rerender"> https://huggingface.co/spaces/Anonymous-sub/Rerender</a>.
        </span>
      </p>

      <br>

<h3 class = "Limitations">
        <span> Limitations </span>
      </h3>
      <p class = "Text">
        <span>
          Only the huggingface demo is avalible at the moment which is very limited and only showcases the keyframe generation ability of 
          Rerender A Video. The Run Propagation button is useless for now and is supposed to be used to generate the whole video when the 
          full code is released. The maximum number of keyframes is also 8 and the maximum frame resolution is 512x768. 
        </span>
        <span>
        The second limitation of note is videos of large or quick motions are unstable. Due to using optical flow, which is only an estimate
        of movement, the model may not be correct if the subject in the video changes position too much or too quickly. 
        </span>
        <span>
        The third limitation that is quite common with any kind of video generation is that it takes quite a long time. The running time of 
        a video of size 512x640 is about 1 minute per keyframe under T4 GPU. From my experience, it takes 2-3 minutes to Run All for 8 keyframes
         and under a minute for the first keyframe. 
        </span>   
      
      <p class = "Text">
        <span>
        I will provide three examples that demonstrate how large or quick motions fail.
        </span>  
        <span>
        The adjectives used in a prompt can significantly impact the generated video from Text2Video-Zero. As an example, I generated 
        two videos using the timlenardo/tdmx-edge-of-realism-dreambooth model, which is a high quality text-to-image model. The left 
        video used the prompt "a cute cat walking on the grass" while the right video used "a cat walking on grass". All other options 
        were kept the same.
        </span>
        <span>
        The following are examples of quick and large motions failing.
        </span>  

        <div class = "video-grid">
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a cute cat walking on grass/timlenardo_tdmx-edge-of-realism-dreambooth.mp4" alt="Video 
            1" controls>
          </div>
                
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a cat walking on grass/timlenardo_tdmx-edge-of-realism-dreambooth.mp4" alt="Video 2" 
            controls>
          </div>
        </div>
      </p>

      <p class="Text">
        <span>
        The difference is clear - the left video shows cat’ s positive face while The right video depicts another angle. This demonstrates 
        how small changes in the prompt wording can guide the model to produce noticeably different results. Carefully choosing descriptive 
        words is an important part of prompt engineering to get better quality and more accurate videos from Text2Video-Zero.
        </span>
      </p>

      <p class = "Text">
        <span>
          What’ s more, using a complete sentence versus a description also impacts the coherence of Text2Video-Zero's output. For example, 
          the prompt "a dog is reading a book" generates a video where the dog's pose and surroundings change inconsistently. However, 
          the prompt "a dog reading a book" produces a more consistent video showing the dog in the same position reading. 
        </span>

        <div class = "video-grid">
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a dog is reading a book/timlenardo_tdmx-edge-of-realism-dreambooth.mp4" alt="Video 1" controls>
          </div>
                
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a dog reading a book/timlenardo_tdmx-edge-of-realism-dreambooth.mp4" alt="Video 2" controls>
          </div>
        </div>
      </p>
      <p class="Text">
        <span>
        The left video varies randomly frame to frame. But the right video depicts the dog continuously reading, by simply describing 
        the subject and action. This shows that prompt phrasing impacts visual coherence, and complete sentences can sometimes confuse 
        the model.
        </span>
      </p>

<h3 class = "Prompt">
        <span> Prompt </span>
      </h3>
      <p class = "Text">
        <span>
          Prompts will change the inputted video a lot and may take a lot of trail and error to get right.
        </span>
        <span>

        </span>
        <span>
          The prompts used in Text2Video-Zero are similar to other text-to-image models. Some examples are:
        </span>   
      
      <p class = "Text">
        <span>
        I will provide two examples that demonstrate how modifying the text prompt results in different generated videos from 
        Text2Video-Zero.
        </span>  
        <span>
        The adjectives used in a prompt can significantly impact the generated video from Text2Video-Zero. As an example, I generated 
        two videos using the timlenardo/tdmx-edge-of-realism-dreambooth model, which is a high quality text-to-image model. The left 
        video used the prompt "a cute cat walking on the grass" while the right video used "a cat walking on grass". All other options 
        were kept the same.
        </span>

        <div class = "video-grid">
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a cute cat walking on grass/timlenardo_tdmx-edge-of-realism-dreambooth.mp4" alt="Video 1" controls>
          </div>
                
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a cat walking on grass/timlenardo_tdmx-edge-of-realism-dreambooth.mp4" alt="Video 2" controls>
          </div>
        </div>
      </p>

      <p class="Text">
        <span>
        The difference is clear - the left video shows cat’ s positive face while The right video depicts another angle. This 
        demonstrates how small changes in the prompt wording can guide the model to produce noticeably different results. Carefully c
        hoosing descriptive words is an important part of prompt engineering to get better quality and more accurate videos from Text2Video-Zero.
        </span>
      </p>

      <p class = "Text">
        <span>
          What’ s more, using a complete sentence versus a description also impacts the coherence of Text2Video-Zero's output. For exa
          mple, the prompt "a dog is reading a book" generates a video where the dog's pose and surroundings change inconsistently. However, the prompt "a dog reading a book" produces a more consistent video showing the dog in the same position reading. 
        </span>

        <div class = "video-grid">
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a dog is reading a book/timlenardo_tdmx-edge-of-realism-dreambooth.mp4" alt="Video 1" controls>
          </div>
                
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a dog reading a book/timlenardo_tdmx-edge-of-realism-dreambooth.mp4" alt="Video 2" controls>
          </div>
        </div>
      </p>
      <p class="Text">
        <span>
        The left video varies randomly frame to frame. But the right video depicts the dog continuously reading, by simply describing th
        e subject and action. This shows that prompt phrasing impacts visual coherence, and complete sentences can sometimes confuse the
         model.
        </span>
      </p>

      <h3 class = "Seed">
        <span> Seed </span>
      </h3>
      <p class = "Text">
        <span>
        A prompt is not all that affects the output of the text to video model. 
        </span>
        <span>
          A seed in generative AI is a starting point or initial input that is used to generate an output. With the same prompt, using
         the same seed will yeild the same results and vice versa.
        </span>
        <span>
          After several experiments on Hugging Face, I have discovered that the subjects you have focused on in your prompt will not 
          change when different seeds are used. In the examples below, I have used the prompt "a man in CG style" with the seeds being 
          different.
        </span>

        <div class = "video-grid">
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a dog reading a book/runwayml_stable-diffusion-v1-5.mp4" alt="Video 1" controls>
          </div>
              
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/an iron man flying in the sky/runwayml_stable-diffusion-v1-5.mp4" alt="Video 2" controls>
          </div>

          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a cute cat walking on grass/runwayml_stable-diffusion-v1-5.mp4" alt="Video 2" controls>
          </div>
        </div>
      </p>

      <p class = "Text">
        <span>
          As you can see, the man's face doesn't change but the background does. In the first example, the background has become a 
          room even though the original video, the background is blurry and doesn't seem like solid walls. In the second example, 
          the background even seems to morph into a wreath that is glued to his head and rest appear to become like a curtain or cloth. 
          And the third example goes back to becoming a room with solid walls. This shows that subjects the prompt doesn't describe will
          experience change that may be added onto the subject the prompt doesn describe.  
        </span>
      </p>

<h3 class = "ControlNet">
        <span> ControlNet </span>
      </h3>
      <p class = "Text">
        <span>
          ControlNet strength affects how much the prompt can affect the output.
        </span>
        <span>
          The higher the ControlNet strength, the less the prompt affects the output, meaning you might not get what you want. I suggest 
          lowering the ControlNet strength to 0.2 if the first keyframe doesn't give you the result you want.
        </span>
        <span>
          I will be demonstrating the effect of ControlNet strength in the follwing examples.
        </span>   
      
      <p class = "Text">
        <span>
        I will provide two examples that demonstrate how modifying the text prompt results in different generated videos from 
        Text2Video-Zero.
        </span>  
        <span>
        The adjectives used in a prompt can significantly impact the generated video from Text2Video-Zero. As an example, I generated 
        two videos using the timlenardo/tdmx-edge-of-realism-dreambooth model, which is a high quality text-to-image model. The left 
        video used the prompt "a cute cat walking on the grass" while the right video used "a cat walking on grass". All other options 
        were kept the same.
        </span>

        <div class = "video-grid">
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a cute cat walking on grass/timlenardo_tdmx-edge-of-realism-dreambooth.mp4" alt="Video 1" controls>
          </div>
                
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a cat walking on grass/timlenardo_tdmx-edge-of-realism-dreambooth.mp4" alt="Video 2" controls>
          </div>
        </div>
      </p>

      <p class="Text">
        <span>
        The difference is clear - the left video shows cat’ s positive face while The right video depicts another angle. This 
        demonstrates how small changes in the prompt wording can guide the model to produce noticeably different results. Carefully c
        hoosing descriptive words is an important part of prompt engineering to get better quality and more accurate videos from Text2Video-Zero.
        </span>
      </p>

      <h3 class = "Subtitle">
        <span> Advanced Options </span>
      </h3>

      <p class = "Text">
        <span>
          Text2Video-Zero provides some advanced options to further control video generation. I will discuss three key ones:
        </span>

        <span>
          Video Length - This specifies the number of frames in the generated 3-4 second video. More frames result in smoother, higher 
          quality videos. Interestingly, increasing video length in Text2Video-Zero improves quality without slowing generation. The le
          ft video has 8 frames, the right has 16 frames. 
        </span>

        <div class = "video-grid">
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a cat walking on grass/8frame.mp4" alt="Video 1" controls>
          </div>
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a cat walking on grass/12frame.mp4" alt="Video 2" controls>
          </div>      
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/a cat walking on grass/16frame.mp4" alt="Video 3" controls>
          </div>
        </div>
      </p>
      <p class = "Text">
        <span>
        As you can see, using more frames consistently produces better visual results without impacting time. Maximum frame length allow
        s the model to create the best possible videos.
        </span>
        <span>
        Global Translation (<math><mrow><mi>&delta;</mi>  
          <msub> <mi>x</mi> </msub>
        </mrow></math>, <math><mrow><mi>&delta;</mi>  
          <msub> <mi>y</mi> </msub>
        </mrow></math>) - These values control the global scene motion and camera movement described in the text prompt. Adjusting <math><mrow><mi>&delta;</mi>  
          <msub> <mi>x</mi> </msub>
        </mrow></math> and <math><mrow><mi>&delta;</mi>  
          <msub> <mi>y</mi> </msub>
        </mrow></math> guides the model to add appropriate left-right and up-down camera motions fitting the caption narrative.
        </span>
        <div class = "video-grid">
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/an astronaut is skiing down the hill/delta12_12.mp4" alt="Video 1" controls>
          </div>
              
          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/an astronaut is skiing down the hill/delta12_16.mp4" alt="Video 2" controls>
          </div>

          <div class ="video-item">
            <video src="/videos/Text2Video-Zero/an astronaut is skiing down the hill/delta12_20.mp4" alt="Video 3" controls>
          </div>
        </div>
      </p>

      <p class = "Text">
        <span>The first three videos demonstrate the effect of changing the <math><mrow><mi>&delta;</mi>  
          <msub> <mi>y</mi> </msub>
        </mrow></math> parameter. The <math><mrow><mi>&delta;</mi>  
          <msub> <mi>y</mi> </msub>
        </mrow></math> values used are 12, 16, and 20.</span>
        <div class = "video-grid">
          <div class = "video-item">
            <video src="/videos/Text2Video-Zero/an astronaut is skiing down the hill/delta16_12.mp4" alt="Video 4" controls></video>
          </div>

          <div class = "video-item">
            <video src="/videos/Text2Video-Zero/an astronaut is skiing down the hill/delta20_12.mp4" alt="Video 5" controls></video>
          </div>
          <div class = "video-item">
            <video src="/videos/Text2Video-Zero/an astronaut is skiing down the hill/delta20_20.mp4" alt="Video 6" controls></video>
          </div>
        </div>
      </p>
      <p class = "Text">
        <span>The next three videos showcase the impact of adjusting the 
          <math><mrow><mi>&delta;</mi>  
            <msub> <mi>x</mi> </msub>
          </mrow></math> parameter. These videos use <math><mrow><mi>&delta;</mi>  
            <msub> <mi>x</mi> </msub>
          </mrow></math> values of 12, 16, and 20.</span>
      </p>

      <p class = "Text">
        <span>
        Carefully tuning these advanced parameters allows generating higher resolution videos with logical camera movements aligned to t
        he descriptive text prompt. Text2Video-Zero enables granular control over video characteristics like length, motion and scene com
        position through both prompt wording and configurable model settings.
        </span>
      </p>

      <br>

      <h3 class = "Subtitle">
        <span> Conclusion </span>
      </h3>

      <p class = "Text">
        <span>
        Collectively, Text2Video-Zero offers you an impressive AI-powered tool for generating realistic videos from text prompts. Hopefu
        lly this tutorial provides guidance for effectively using Text2Video-Zero.
        </span>
      </p>

      <h3 class = "Subtitle">
        <span> Reference </span>
      </h3>

      <p class = "Text">
        <span>
          Khachatryan, L. (2023, March 23). Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators. arXiv.org. <a href = "https://arxiv.org/abs/2303.13439">https://arxiv.org/abs/2303.13439 </a>
        </span>
      </p>

    </article>

  </div>
</body>
