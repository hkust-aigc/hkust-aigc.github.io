<!DOCTYPE html>
<html lang = "en-us">

<head>
  <meta charset="utf-8">
  <meta name = "description" content="A studio that shows videos produced by generative AI">
  <meta name = "keywords" content="Generative AI,video,github">
  <meta name = "viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=yes">
  <title> HKUST AIGC Studio</title>

  <link rel = "stylesheet" href = "/css/reset.css">
  <link rel = "stylesheet" href = "/css/pageStyle.css">
  <link rel = "stylesheet" href = "/css/topbar.css">
  <link rel = "stylesheet" href = "/css/tutorial.css">
  <link rel = "stylesheet" href = "/css/videoSytle.css">

  <style>
    ul li {
      font-size: 1.2em;
      margin-bottom: 1em;
      align-items: left; 
    }
  </style>
 
</head>

<body>
  
  <!--top navigation-->

  <div class = "topbar">

    <div class = "container clearfix">
      <!--logo-->

      <div class = "topbar-icon leftfix">
        <a href = "/"> <img class = "logo" src="/images/logo.png" alt="HKUST AIGC logo"> </a>
      </div>

      <!--navigation-->

      <div class = "topbar-navigation rightfix">
        <ul class = "list clearfix">
          <li> 
            <a href= "/explore"> Explore</a> 
          </li>
          <li> 
            <a href="/showcase" > Showcase </a> 
          </li>
        </ul>

        <div class = "Creator">
          <a href="/videoCreators"> 
            <button> Create </button>
          </a> 
        </div>

        <div class = "searchBox">
          <form action = "/search/creations/">
            <input type = "text" name = "searchText" placeholder = "Search HKUST AIGC Studio">
            <button> search </button>
          </form> 
        </div>
        
      </div>

    </div>
  </div>

  <!--Never modify the above code to avoid ruining the website-->

  <div class = "tutorial">

    <!--The following is what you need to modify-->
    <!--This a template which contains title, subtitle and text part of your article-->
    <!---->

    <article> 
      <h2 class = "Title"> VideoCrafter </h2>

      <!--The following includes a subtitle and a text-->
      <h3 class = "Subtitle"> 
        <span> Model decomposition</span>
      </h3>
      
      <p class = "Text"> 
        <span> 
          <!-- &nbsp is a character entity reference that stands for "non-breaking space."-->
          &nbsp; &nbsp; VideoCrafter is a video creation toolbox consisting of:
            <ul>
                <li> A text-to-video (T2V) diffusion model : LVDM  
                <li> A finetuning mechanism to train the LVDM on specific datasets and reduce training parameters : VideoLoRA 
                <li> A conditional adapter based on T2I-adpater to increase the controls on video generation
            </ul>
         <h2> VideoCrafter has the interesting feature of allowing long-form video generation. The mechanisms used to achieve this are interesting, as typically diffusion models fail for longer form content generation. Using autoregressive latent prediction and unconditional guidance, techniques which are further described in <a href ="https://arxiv.org/pdf/2211.13221.pdf">the original LVDM paper.</a> </h2>
        </span>
      </p>
    <br>
        <h3 class = "Subtitle"> <!-- Amadika : Add subsection to the stylesheet if you want --> 
            <span> How to Use VideoCrafter </span>
        </h3>
        <p class = "Text">
        <span>
          &nbsp; &nbsp; There are a couple of ways you can play around with VideoCrafter. You can run it locally, run a <a href = "https://huggingface.co/VideoCrafter">demo on HuggingFace</a>, <a href = "https://colab.research.google.com/github/VideoCrafter/VideoCrafter/blob/main/quick_demo.ipynb">the Google Colabatory notebook</a> or on <a href = "https://replicate.com/cjwbw/videocrafter">Replicate.</a>
          <br>
          Prompt engineering : prompt engineering is the most important parameter when it comes to generative models. Wriitng a descriptive prompt which covers most or all different elements of the desired output. A good resource to understand further is <a href="https://stable-diffusion-art.com/how-to-come-up-with-good-prompts-for-ai-image-generation/" >here </a>. <br>

          The HuggingFace demo of VideoCrafter shows us the features we have available to play around with. <br> <img src= "/videos/VideoCrafter/Screenshot 2023-08-07 at 10.17.54.png">
          <ul>
              <li>
                Sampling steps : the number of sampling steps used adjusts how much the diffusion model denoises the data (which in a diffusion model is 100% random noise) per step. At every step the model is predicting what the image is supposed to look like. Adding more steps implies that execution will take longer but at every step the model is making a probabilistic assumption based on a less noisy image.
                
              </li>
            <h2> Prompt: "teddy bear playing the piano, high definition, 4k", all other settings default</h2>
              <div class = "video-grid">
              
                <div class = "video-item">
                  <video src = "/videos/VideoCrafter/teddy_bear_playing_piano_high_definition_4k_1.mp4" alt = "1 Sampling Step" controls></video> 
                  <h2> 1 sampling step </h2>
                </div>
              <div class = "video-item">
                  <video src = "/videos/VideoCrafter/teddy_bear_playing_piano_high_definition_4k_10.mp4" alt = "10 Sampling Step" controls> </video>
                  <h2> 10 sampling step </h2>
                  </div>
              <div class = "video-item">
                  <video src = "/videos/VideoCrafter/tddy50.mp4" alt = "50 sampling" controls></video>
                  <h2> 50 sampling step </h2>
                  </div>
              </div>
            <!-- Mention in the paper where this comes up when you're talking about variance in the diffusion process -->
              <li> CFG scale : Stands for Coefficient of Fluctuation Growth. Adjusting this scale adjusts how faithful the generated output is to the prompt. It quantifies the rate of change of the variance of a diffusion process. 
              </li> 
           
              <li> eta (η) : weight of noise for added noise in diffusion step. This is a parameter used in the reverse process of the diffusion model. </li>
              <li> LoRA scale : LoRA is a finetuning framework that can be used to train the base diffusion model on more specialised datasets to generate results with different features. The LoRA scale on the model is only useful if using a LoRA model.</li>
            <h2> LoRA demonstration; Prompt : "black cat eating ice cream, animated", sampling steps = 50 </h2>
            <div class = "video-grid">
              
                <div class = "video-item">
                  <video src = "/videos/VideoCrafter/replicate-prediction-wzimsarbpqi73et3niu3enhzie.mp4" alt = "1 Sampling Step" controls> </video>
                  <h2> Default model </h2>
                </div>
              <div class = "video-item">
                  <video src = "/videos/VideoCrafter/frozen16.mp4" alt = "10 Sampling Step" controls> </video>
                  <h2> Frozen movie style, LoRA 1.6 </h2>
                  </div>
              <div class = "video-item">
                  <video src = "/videos/VideoCrafter/vincent16.mp4" alt = "50 sampling" controls></video>
                  <h2> Vincent Van Gogh style, LoRA 1.6 </h2>
                  </div>
              <div class = "video-item">
                  <video src = "/videos/VideoCrafter/vincent2.mp4" alt = "50 sampling" controls></video>
                  <h2> Vincent Van Gogh style, LoRA 2 </h2>
                  </div>
              </div>
          </ul>
      </span>
        
            
      </p>


      <h3 class = "Subtitle">
        <span> Diffusion models </span>
      </h3>
      <p class = "Text">
        <span>
          &nbsp; &nbsp; Diffusion models are among the more mathematically involved generative models. For more explanation on the math behind these models, see <a href="https://docs.google.com/document/d/1NKoy3RDuaZDxErRxup23iSF2cxetZocSf_t-n1R1Hn8/edit?usp=sharing">this doc.</a> <br>
          Latent diffusion models are a type of machine learning model that map the characteristics of a dataset to a lower-dimensional latent space through an encoder. 
          A key intuition when considering how machine learning models is : it is desired that every arbitrary data sample can be represented by a well-understood distribution so that established properties can be used to make computations more efficient. <br>
          
          Diffusion models "generate" by "diffusing" an existing image of a certain class into complete noise, and then un-diffusing it into a sensible sample. This ends up generating a new image as the calculations are approximate and we make smart assumptions to preserve the structure of the image while making calculations more efficient. 
  
      In an attempt to make this less hand-wavey, let's take a bird-eye's view of the mathematics. 
      We have a data point which we will use to convert into an image of the same category. 
      Let's say this data point, x_0 resembles a distribution q(x). We assume that the process of adding noise to this distribution is a Markovian process, and we model it by q(x_t | x_{t-1}).
      We fix a variance schedule, {β} that influences the variance in the noisiness between timesteps. We fix T of these, where T is the chosen number of sampling steps. By the end of this forward process, we've converted the sample to pure noise. <br>
          We can now start generating a never-seen-before image out of this noise. If we try to take the posterior of the previous Markovian process, if the covariance between the distributions at two consecutive timesteps is small enough, it can be shown that the posterior follows approximately the same distribution as the original process. Thus we have an approximate posterior, called p(x_{t-1} | x_t), and the real distribution q(x_{t-1} | x_t). We will leverage the distribution p(x_t) till we can remove all the noise from the sample and resolve it to a new image. Different diffusion models use different models of p. If we try to model p as close to q as possible, p would be a Markovian process as well and be computationally heavy. This is the concept of Denoising Diffusion Probabilistic Models (DDPM). Denoising Diffusion Implicit Models, as used in VideoCrafter, ease computational complexity by assuming a non-Markovian reverse process distribution. <br>

        The training objective of diffusion models is an optimisation function comparing the approximate distribution p and the actual reverse distribution q. Since we cannot possibly compute the real values of these effectively, we use a variational lower bound (as in VAEs), using KL Divergence and cross-entropy. If you wish to learn more about this, you may want to check out <a href = "https://nlp.stanford.edu/fsnlp/mathfound/fsnlp-slides-kl.pdf">this link</a> and then <a href = "https://jaketae.github.io/study/elbo/"> this one.</a>
        </span>
      </p>

      <br>

      <h3 class = "Subtitle">
        <span> Inferences </span>
      </h3>
      <p class = "Text">
        <span>
          &nbsp; &nbsp; In my observations with VideoCrafter, it does a good job of rendering simple prompts. Diffusion models in general seem to be good at identifying and generating images around one main subject and unless in simple cases, fail to generate multiple complex elements. This would make sense considering the mechanism as explained above. 
        </span>
      </p>
      
      <!--
        You can use the below code part to show your demo videos in the sytle that the website adopts
      -->
      
      <!-- <div class = "video-grid">
        <div class ="video-item">
          <video src="your_video1.mp4" alt="Video 1" controls>
        </div>
            
        <div class ="video-item">
          <video src="your_video2.mp4" alt="Video 2" controls>
        </div>
      </div> -->

      <br>
      
      <h3 class = "Subtitle">
        <span> References </span>
      </h3>
      <p class = "Text">
        <span>
          &nbsp; &nbsp;He, Y., Yang, T., Zhang, Y., Shan, Y. & Chen, Q. 2023 Latent Video Diffusion Models for High-Fidelity Long Video Generation. arXiv:2211.13221 https://arxiv.org/abs/2211.13221 <br>
          Goodfellow, I., Pouget-Abadie, J., Mirza M., Xu, B., Warde-Farley, D., Qzair, S., Courville, A. & Bengio, Y. 2014 General Adversarial Networks arXiv:1406.2661 https://arxiv.org/abs/1406.2661 <br>
Feller, W. 1949 On The Theory of Stochastic Processes With Particular Reference to Applications Project for Research in Probability <br>
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. 2015 Deep Unsupervised Learning using Nonequilibrium Thermodynamics arXiv:1503.03585 https://arxiv.org/abs/1503.03585 <br>
Blattman, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S., Fidler, S. & Kreis, K. 2023 Align Your Latents. IEEE Conference on Computer Vision and Pattern Recognition. arXiv:2304.08818  https://arxiv.org/abs/2304.08818 <br>

Graikos, AYellapragada S. & Samaras, D. 2023 Conditional Generation from Unconditional Diffusion Models using Denoiser Representations. arXiv:2306.01900 https://arxiv.org/pdf/2306.01900.pdf 
        </span>
      </p>

      <!--It' s up to you to write more-->

    </article>

  </div>
</body>
