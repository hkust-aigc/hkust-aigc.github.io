<!DOCTYPE html>
<html lang = "en-us">

<head>
  <meta charset="utf-8">
  <meta name = "description" content="A studio that shows videos produced by generative AI">
  <meta name = "keywords" content="Generative AI,video,github">
  <meta name = "viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=yes">
  <title> HKUST AIGC Studio</title>

  <link rel = "stylesheet" href = "/css/reset.css">
  <link rel = "stylesheet" href = "/css/pageStyle.css">
  <link rel = "stylesheet" href = "/css/topbar.css">
  <link rel = "stylesheet" href = "/css/tutorial.css">
  <link rel = "stylesheet" href = "/css/videoSytle.css">
</head>

<body>
  
  <!--top navigation-->

  <div class = "topbar">

    <div class = "container clearfix">
      <!--logo-->

      <div class = "topbar-icon leftfix">
        <a href = "/"> <img class = "logo" src="/images/logo.png" alt="HKUST AIGC logo"> </a>
      </div>

      <!--navigation-->

      <div class = "topbar-navigation rightfix">
        <ul class = "list clearfix">
          <li> 
            <a href= "/explore"> Explore</a> 
          </li>
          <li> 
            <a href="/showcase" > Showcase </a> 
          </li>
        </ul>

        <div class = "Creator">
          <a href="/videoCreators"> 
            <button> Create </button>
          </a> 
        </div>

        <div class = "searchBox">
          <form action = "/search/creations/">
            <input type = "text" name = "searchText" placeholder = "Search HKUST AIGC Studio">
            <button> search </button>
          </form> 
        </div>
        
      </div>

    </div>
  </div>

  <!--Never modify the above code to avoid ruining the website-->

  <div class = "tutorial">

    <!--The following is what you need to modify-->
    <!--This a template which contains title, subtitle and text part of your article-->
    <!---->

    <article> 
      <h2 class = "Title"> Drag GAN - Interactive Point-based Manipulation on the Generative Image Manifold </h2>
      <h3 class = "Author"> Muzi ZHANG </h3>

      <!--The following includes a subtitle and a text-->
      <h3 class = "Subtitle"> 
        <span> Introduction </span>
      </h3>
      <p class = "Text">
        <!--It is also recommend to wrap each paragraph with <span> tags.--> 
        <span> 
          <!-- &nbsp is a character entity reference that stands for "non-breaking space."-->
          &nbsp; &nbsp; Drag GAN is a new kind of GAN that allows user to modify multiple features such as shape, body pose, emotion of the images in a generative approach, 
            users can achieve this by adding one more multiple pairs of points and DragGAN will perform a series of iterations that "drag" the handle points to the corresponding 
            target points without modifying pixels outside a region specified by the user.
        </span>
      </p>

      <!--You can copy the above part yourself to write the remaining parts-->

      <br> <!--It is used to create a line break or a new line-->

      <h3 class = "Subtitle">
        <span> How to use </span>
      </h3>
      <p class = "Text">
        <span>
          &nbsp; &nbsp; DragGAN include a GUI and users can do the image editing by simply clicking the image, then DragGAN will do multiple iterations so that the
            image is transform slowly into the target.
        </span>
        <img src="/images/DragGAN/DragGAN.gif" alt="model" style="display: block; margin:auto; max-width: 60%">
        
        <span>
          &nbsp; &nbsp; During the transformation, you can also stop at any step and modify the handle and target point(s), so that the image transformation can go on 
            with a different target. <br> <br>

            &nbsp; &nbsp;Since DragGAN is based on the StyleGAN2 model and perform image editing by modifying the mapped latent vector w of StyleGAN2 instead of doing this directly
              on the input image, this tool can only handle generated images from StyleGAN2. Therefore, if user want to upload their own image, an extra GAN Inversion that 
              transform the image back to latent vector is required, which can probably affect the details of the original image.
        </span>
        <img src="/images/DragGAN/demo1.png" alt="model" style="display: block; margin:auto; max-width: 60%">
      </p>
      
      <!--
        You can use the below code part to show your demo videos in the sytle that the website adopts
      -->
      
      <!-- <div class = "video-grid">
        <div class ="video-item">
          <video src="your_video1.mp4" alt="Video 1" controls></video>
          <h2> your word(optional) </h2>
        </div>
            
        <div class ="video-item">
          <video src="your_video2.mp4" alt="Video 2" controls> </video>
            <h2> your word(optional) </h2>
        </div>
      </div> -->

      <br>

      <h3 class = "Subtitle">
        <span> Technical details </span>
      </h3>
      <p class = "Text">
        <span>
          &nbsp; &nbsp; This tool is based on the StyleGAN2 architecture [Karras et al. 2020]. <br>
          &nbsp; &nbsp; Different to common GAN models, the 512 dimentional latent vector ğ’› âˆˆ N(0, ğ‘° ) is mapped to an intermediate
          latent code ğ’˜ via a mapping network in the StyleGAN2 architecture, which is used to control the style of the image. Then this vector
          is fed to every layer in the synthesis network, with an addtional noise vector. 
        </span>
        <img src="/images/DragGAN/demo2.jpg" alt="model" style="display: block; margin:auto; max-width: 60%">
        <span>
          &nbsp; &nbsp; DragGAN will do optimization base on the latent vector ğ’˜ so that the handle points can move closer to the target point. 
          To move a handle point ğ’‘ğ‘– to the target ğ’•ğ‘–, the DragGAN supervise a small region around ğ’‘ğ‘– towards ğ’•ğ‘–, and design the loss function like this:
        </span>
        <img src="/images/DragGAN/loss.png" alt="model" style="display: block; margin:auto; max-width: 60%">
        <span>
          &nbsp; &nbsp; where F(ğ’’) denotes the feature values of F at pixel ğ’’, ğ’…ğ‘– is a normalized vector pointing from ğ’‘ğ‘– to ğ’•ğ‘– (ğ’…ğ‘– = 0 if ğ’•ğ‘– = ğ’‘ğ‘– ),
          and F0 is the feature maps corresponding to the initial image. Note
          that the first term is summed up over all handle points {ğ’‘ğ‘– }. As the
          components of ğ’’ğ‘– +ğ’…ğ‘– are not integers, we obtain F(ğ’’ğ‘– +ğ’…ğ‘– ) via bilinear
          interpolation.
        </span>

      </p>

      <h3 class = "Subtitle">
        <span> References </span>
      </h3>
      <p class = "Text">
        <span>
          Xingang Pan [2023, May 18] Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold. https://arxiv.org/abs/2305.10973
        <br>Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De
        Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero
        Karras, and Gordon Wetzstein. 2022. Efficient Geometry-aware 3D Generative
        Adversarial Networks. In CVPR.
        </span>
      </p>

      <!--It' s up to you to write more-->

    </article>

  </div>
</body>
